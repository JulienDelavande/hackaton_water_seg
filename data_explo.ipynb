{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_IMAGE_TRAIN = 'dataset/trainset/images'\n",
    "FOLDER_MASK_TRAIN = 'dataset/trainset/masks'\n",
    "\n",
    "FOLDER_IMAGE_TEST = 'dataset/testset/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 891\n",
      "Number of masks: 891\n"
     ]
    }
   ],
   "source": [
    "# see if each iamge has a mask\n",
    "image_names = []\n",
    "mask_names = []\n",
    "for image in os.listdir(FOLDER_IMAGE_TRAIN):\n",
    "    image_names.append(image.split('.')[0])\n",
    "for image in os.listdir(FOLDER_MASK_TRAIN):\n",
    "    mask_names.append(image.split('.')[0])\n",
    "\n",
    "print('Number of images:', len(image_names))\n",
    "print('Number of masks:', len(mask_names))\n",
    "\n",
    "# check if each image has a mask\n",
    "for image in image_names:\n",
    "    if image not in mask_names:\n",
    "        print('Image', image, 'has no mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaterSegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(images_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        mask_path = os.path.join(self.masks_dir, img_name.replace('.jpg', '.png'))\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = nn.MaxPool2d(2)\n",
    "        self.conv1 = DoubleConv(64, 128)\n",
    "        self.down2 = nn.MaxPool2d(2)\n",
    "        self.conv2 = DoubleConv(128, 256)\n",
    "        self.down3 = nn.MaxPool2d(2)\n",
    "        self.conv3 = DoubleConv(256, 512)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv4 = DoubleConv(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv5 = DoubleConv(256, 128)\n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv6 = DoubleConv(128, 64)\n",
    "\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x2 = self.conv1(x2)\n",
    "        x3 = self.down2(x2)\n",
    "        x3 = self.conv2(x3)\n",
    "        x4 = self.down3(x3)\n",
    "        x4 = self.conv3(x4)\n",
    "\n",
    "        x5 = self.up1(x4)\n",
    "        x5 = torch.cat([x5, x3], dim=1)\n",
    "        x5 = self.conv4(x5)\n",
    "        x6 = self.up2(x5)\n",
    "        x6 = torch.cat([x6, x2], dim=1)\n",
    "        x6 = self.conv5(x6)\n",
    "        x7 = self.up3(x6)\n",
    "        x7 = torch.cat([x7, x1], dim=1)\n",
    "        x7 = self.conv6(x7)\n",
    "\n",
    "        logits = self.outc(x7)\n",
    "        return logits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
